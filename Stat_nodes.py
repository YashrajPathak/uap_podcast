"""
nodes.py â€” Core LLM-powered functions for StatAgent.

This module handles:
- Generating Stat's data integrity and validation responses
- Processing conversation context and Reco's previous statements
- Preparing prompt construction and interaction logic
"""

import asyncio
from typing import List, Dict, Any

from uap_podcast.models.podcast import llm
from uap_podcast.agents.stat_agent.utils.state import SYSTEM_STAT
from uap_podcast.agents.stat_agent.utils.tools import (
    ensure_complete_response,
    vary_opening,
    add_conversation_dynamics,
    add_emotional_reactions,
    clean_repetition
)


async def generate_stat_response(
    context: str,
    nexus_intro: str,
    reco_last_response: str,
    conversation_history: List[str],
    last_openings: Dict[str, str],
    last_speaker: str,
    turn_index: int
) -> str:
    """
    ðŸ“Š Generate a StatAgent response based on context, Nexus intro, and Recoâ€™s last statement.

    Args:
        context: Full dataset context from input files.
        nexus_intro: Introduction text generated by Nexus agent.
        reco_last_response: The most recent recommendation statement from Reco.
        conversation_history: Previous conversation lines for continuity.
        last_openings: Dict tracking last used openers to avoid repetition.
        last_speaker: Last speaker's name to shape conversational dynamics.
        turn_index: Current turn count (used for variation).

    Returns:
        str: Final, polished StatAgent response.
    """

    # Construct the LLM prompt for StatAgent
    stat_prompt = (
        f"Context: {context}\n\n"
        f"Nexus introduced these topics: {nexus_intro}\n\n"
        f"Reco just said: {reco_last_response}\n\n"
        f"Previous conversation: {conversation_history[-3:] if len(conversation_history) >= 3 else 'None'}\n\n"
        f"Respond to Recoâ€™s point, focusing on data integrity, validation, and risk assessment aspects."
    )

    # Generate raw LLM response
    response = await llm(SYSTEM_STAT, stat_prompt)

    # âœ¨ Post-process for naturalness, dynamics, and clarity
    response = vary_opening(response, "STAT", last_openings)
    response = add_conversation_dynamics(response, "STAT", last_speaker, context, turn_index, conversation_history)
    response = add_emotional_reactions(response, "STAT")
    response = clean_repetition(response)
    response = ensure_complete_response(response)

    return response


# ---------- Optional utility wrapper for quick testing ----------

if __name__ == "__main__":
    async def _test_stat_node():
        sample_context = "[Demo] Sample metric data context..."
        sample_intro = "Todayâ€™s focus is on ASA trends and processing time anomalies."
        reco_response = "Given the volatility, we should consider a 3-month rolling average to smooth ASA."
        history = ["Nexus: Welcome", "Reco: Let's start.", f"Reco: {reco_response}"]
        result = await generate_stat_response(
            context=sample_context,
            nexus_intro=sample_intro,
            reco_last_response=reco_response,
            conversation_history=history,
            last_openings={},
            last_speaker="Reco",
            turn_index=1
        )
        print("Stat Response:", result)

    asyncio.run(_test_stat_node())
